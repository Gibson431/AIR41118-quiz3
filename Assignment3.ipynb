{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r63rpM7Sff2b"
      },
      "source": [
        "# Assignment 3: Reinforcement Learning\n",
        "In this assignment you will apply the RL algorithms you learnt from the tutorials to a simulated robot car in a pybullet environment.\n",
        "\n",
        "You will be asked to (percentages are allocation of assignment marks):\n",
        "* Train the robot to drive to the green goal marker which spawns at random locations (60%)\n",
        "* Modify the epsilon-greedy function to incorporate prior knowledge (20%)\n",
        "* Modify the reward function (10%)\n",
        "* Add obstacles to the environment (10%)\n",
        "\n",
        "It is highly recommended to install pybullet and run your code locally since things will run much faster. It will also make editing the gym environment code easier.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx8knrhyhteV"
      },
      "source": [
        "## Simple Car Environment\n",
        "---\n",
        "\n",
        "![simple_car.gif](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExODU0NmVlMzU1MGU1YzJjMjA5ODE5NjM0MTg0MTU1ZmM1OTA1NzRkNCZjdD1n/VI3OuvQShK3gzENiVz/giphy.gif)\n",
        "\n",
        "*(code for this tutorial adapted from: https://gerardmaggiolino.medium.com/creating-openai-gym-environments-with-pybullet-part-2-a1441b9a4d8e*)\n",
        "\n",
        "---\n",
        "\n",
        "This is a simple car environment with a continuous state space and discrete action space with the goal of driving towards a green marker. Driving within 1.5 metres of the green marker causes the episode to end or if a certain amount of time has passed.\n",
        "\n",
        "We can instantiate the environment as follows:\n",
        "\n",
        "\n",
        "```\n",
        "env = gym.make('SimpleDriving-v0', apply_api_compatibility=True, renders=False, isDiscrete=True, render_mode='tp_camera')\n",
        "```\n",
        "\n",
        "\n",
        "### Action Space\n",
        "\n",
        "*   0: Reverse-Left\n",
        "*   1: Reverse\n",
        "*   2: Reverse-Right\n",
        "*   3: Steer-Left (no throttle)\n",
        "*   4: No throttle and no steering\n",
        "*   5: Steer-Right (no throttle)\n",
        "*   6: Forward-right\n",
        "*   7: Forward\n",
        "*   8: Forward-left\n",
        "\n",
        "### Observation Space\n",
        "Two dimensional array with distance in (x, y) from goal position.\n",
        "\n",
        "###Rewards\n",
        "Negative euclidean distance from the goal.\n",
        "\n",
        "### Interacting with the Environment\n",
        "We can sample actions randomly, get the agent to perform that action and then observe how the environment state changes:\n",
        "```\n",
        "state = env.reset()  # this needs to be called once at the start before sending any actions\n",
        "action = env.action_space.sample()\n",
        "state, reward, done, _, info = env.step(action)\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Installing and Modifying Gym Environment Code\n",
        "\n",
        "For installing in collab you would have already been familiar with using the following command:\n",
        "```\n",
        "pip install git+https://github.com/fredsukkar/simple-car-env-template\n",
        "```\n",
        "\n",
        "To edit the gym environment first create a github account and then go to https://github.com/fredsukkar/simple-car-env-template and create a new repository using the repository as a template as follows:\n",
        "![sdlfk](https://i.ibb.co/MMsLv1G/github-template.jpg)\n",
        "\n",
        "\n",
        "Once you have your own copy of the repository you can then edit the files in the browser via github or alternatively (recommended) you can [clone the repository](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) and modify the code locally.\n",
        "\n",
        "To install the package from github you can use the usual:\n",
        "```\n",
        "pip install git+https://github.com/<your repo>\n",
        "```\n",
        "\n",
        "To install the package locally:\n",
        "```\n",
        "cd /path/to/cloned_repo\n",
        "python setup.py install\n",
        "```\n",
        "\n",
        "Note that for both methods you will need to install the package again after you've made any changes for them to take any effect.\n",
        "\n",
        "The main file you will be modifying is: https://github.com/fredsukkar/Gym-Medium-Post/blob/main/simple_driving/envs/simple_driving_env.py.\n",
        "\n",
        "There are four main functions that you have been calling via the gym environment object:\n",
        "```\n",
        "class SimpleDrivingEnv(gym.Env):\n",
        "    metadata = {'render.modes': ['human']}  \n",
        "  \n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def step(self, action):\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        pass\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "```\n",
        "\n",
        "Parts 3 and 4 of the assignment will ask you to modify one of these functions.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1j7Dqubpfql"
      },
      "source": [
        "Before we can execute any code we first need to install the following packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "OzqrVWfmZIqa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Found existing installation: simple_driving 0.0.1\n",
            "Uninstalling simple_driving-0.0.1:\n",
            "  Successfully uninstalled simple_driving-0.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Processing /home/main/Documents/git/AIR41118-quiz3\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: gym in /home/main/.local/lib/python3.12/site-packages (from simple_driving==0.0.1) (0.26.2)\n",
            "Requirement already satisfied: pybullet in /home/main/.local/lib/python3.12/site-packages (from simple_driving==0.0.1) (3.2.6)\n",
            "Requirement already satisfied: numpy in /home/main/.local/lib/python3.12/site-packages (from simple_driving==0.0.1) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /home/main/.local/lib/python3.12/site-packages (from simple_driving==0.0.1) (3.8.4)\n",
            "Requirement already satisfied: torch in /home/main/.local/lib/python3.12/site-packages (from simple_driving==0.0.1) (2.2.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /home/main/.local/lib/python3.12/site-packages (from gym->simple_driving==0.0.1) (3.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /home/main/.local/lib/python3.12/site-packages (from gym->simple_driving==0.0.1) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /home/main/.local/lib/python3.12/site-packages (from matplotlib->simple_driving==0.0.1) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/main/.local/lib/python3.12/site-packages (from matplotlib->simple_driving==0.0.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /home/main/.local/lib/python3.12/site-packages (from matplotlib->simple_driving==0.0.1) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /home/main/.local/lib/python3.12/site-packages (from matplotlib->simple_driving==0.0.1) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3.12/site-packages (from matplotlib->simple_driving==0.0.1) (23.1)\n",
            "Requirement already satisfied: pillow>=8 in /usr/lib64/python3.12/site-packages (from matplotlib->simple_driving==0.0.1) (10.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3.12/site-packages (from matplotlib->simple_driving==0.0.1) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/lib/python3.12/site-packages (from matplotlib->simple_driving==0.0.1) (2.8.2)\n",
            "Requirement already satisfied: filelock in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (4.11.0)\n",
            "Requirement already satisfied: sympy in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (2024.3.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/main/.local/lib/python3.12/site-packages (from torch->simple_driving==0.0.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/main/.local/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->simple_driving==0.0.1) (12.4.127)\n",
            "Requirement already satisfied: six>=1.5 in /usr/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib->simple_driving==0.0.1) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib64/python3.12/site-packages (from jinja2->torch->simple_driving==0.0.1) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /home/main/.local/lib/python3.12/site-packages (from sympy->torch->simple_driving==0.0.1) (1.3.0)\n",
            "Building wheels for collected packages: simple_driving\n",
            "  Building wheel for simple_driving (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for simple_driving: filename=simple_driving-0.0.1-py3-none-any.whl size=7931 sha256=78b0a5114de6d228506b2d97fe8fa4dcdf471b4c97e56382995cbf414784f854\n",
            "  Stored in directory: /home/main/.cache/pip/wheels/3a/d2/2d/0aa60793e7610d5e1edc1078a8f47425f63c436336a7b037a6\n",
            "Successfully built simple_driving\n",
            "Installing collected packages: simple_driving\n",
            "Successfully installed simple_driving-0.0.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# !apt update\n",
        "%pip install -U gym==0.26.2 pyvirtualdisplay pygame torch > /dev/null 2>&1\n",
        "%pip uninstall simple_driving -y\n",
        "%pip install -U .\n",
        "# !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "# !apt-get install -y xvfb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqiZQ4hUp3lv"
      },
      "source": [
        "Now import the necessary packages and following helper functions (you don't need the `display_video` function if running locally):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3xeRJtf7p_q1"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import pybullet as p\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import HTML\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "import numpy as np\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import pickle\n",
        "from IPython.display import clear_output\n",
        "import torch\n",
        "import random\n",
        "\n",
        "import simple_driving\n",
        "from simple_driving.envs.simple_driving_env import SimpleDrivingEnv\n",
        "\n",
        "\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()\n",
        "\n",
        "def display_video(frames, framerate=30):\n",
        "  \"\"\"Generates video from `frames`.\n",
        "\n",
        "  Args:\n",
        "    frames (ndarray): Array of shape (n_frames, height, width, 3).\n",
        "    framerate (int): Frame rate in units of Hz.\n",
        "\n",
        "  Returns:\n",
        "    Display object.\n",
        "  \"\"\"\n",
        "  height, width, _ = frames[0].shape\n",
        "  dpi = 70\n",
        "  orig_backend = matplotlib.get_backend()\n",
        "  matplotlib.use('Agg')  # Switch to headless 'Agg' to inhibit figure rendering.\n",
        "  fig, ax = plt.subplots(1, 1, figsize=(width / dpi, height / dpi), dpi=dpi)\n",
        "  matplotlib.use(orig_backend)  # Switch back to the original backend.\n",
        "  ax.set_axis_off()\n",
        "  ax.set_aspect('equal')\n",
        "  ax.set_position([0, 0, 1, 1])\n",
        "  im = ax.imshow(frames[0])\n",
        "  def update(frame):\n",
        "    im.set_data(frame)\n",
        "    return [im]\n",
        "  interval = 1000/framerate\n",
        "  anim = animation.FuncAnimation(fig=fig, func=update, frames=frames,\n",
        "                                  interval=interval, blit=True, repeat=False)\n",
        "  return HTML(anim.to_html5_video())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps9E66nS-Cr7"
      },
      "source": [
        "## Part 1\n",
        "\n",
        "Train the robot to drive to the green goal marker. Use any of the RL algorithms you learnt in the tutorials.\n",
        "\n",
        "You can save the model after training to save you having to retrain everytime you open colab:\n",
        "```\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "torch.save(model.state_dict(), \"/content/drive/My Drive/Colab Notebooks/simple_driving_qlearning.pkl\")  # this will save to folder \"Colab Notebooks\" on your google drive\n",
        "```\n",
        "\n",
        "You can then load the model:\n",
        "```\n",
        "model.load_state_dict(torch.load(\"/content/drive/My Drive/Colab Notebooks/simple_driving_qlearning.pkl\"))\n",
        "```\n",
        "\n",
        "Once loaded you can even continue your training again using the learnt weights, that way you don't have to start from scratch again if you decide you want to train for longer. But keep in mind the epsilon greedy function will start from completely random again so you might want to set epsilon starting value appropriately."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXODTRa7_WAz"
      },
      "source": [
        "## Part 2\n",
        "\n",
        "Incorporate prior knowledge into the epsilon-greedy function by choosing a non-uniform distribution to sample from when performing exploration. For example, for training flappy bird we used the following to sample flapping actions less often to avoid flying off into the sky during early stages of training:\n",
        "\n",
        "```\n",
        "return np.random.choice(np.array(range(2)), p=[0.9,0.1])\n",
        "```\n",
        "\n",
        "Note that you will need to change the parameters to suit the car's action space and also choose a suitable distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XasnJTZ2Bynb"
      },
      "source": [
        "## Part 3\n",
        "\n",
        "Modify the reward to give a bonus of 50 if the goal is reached. You can do this either in the `simulate` function or directly by modifying the `step` function in the gym environment code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztxrzwvMCfnj"
      },
      "source": [
        "## Part 4\n",
        "\n",
        "Add obstacles to the environment. You can do this by modifying the `reset` function in the gym environment code. For example you can add objects as follows:\n",
        "```\n",
        "self.obstacle = self._p.loadURDF(fileName=<path to urdf file here>,\n",
        "                   basePosition=[0, 0, 0])\n",
        "```\n",
        "\n",
        "An example urdf file: https://github.com/fredsukkar/simple-car-env-template/blob/main/simple_driving/resources/simplegoal.urdf\n",
        "\n",
        "**Note:** you will need to add features to your state so that the agent learns to avoid obstacles. For example, you could add the x, y distance from the agent to the closest obstacle in the environment. Then your state would become: `[x_goal, y_goal, x_obstacle, y_obstacle]`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqdyWkJWB40I"
      },
      "source": [
        "Here is some code to help you get started."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyper parameters that will be used in the DQN algorithm\n",
        "\n",
        "EPISODES = 5000                 # number of episodes to run the training for\n",
        "LEARNING_RATE = 0.00025         # the learning rate for optimising the neural network weights\n",
        "MEM_SIZE = 50000                # maximum size of the replay memory - will start overwritting values once this is exceed\n",
        "REPLAY_START_SIZE = 10000       # The amount of samples to fill the replay memory with before we start learning\n",
        "BATCH_SIZE = 64                 # Number of random samples from the replay memory we use for training each iteration\n",
        "GAMMA = 0.99                    # Discount factor\n",
        "EPS_START = 0.1                 # Initial epsilon value for epsilon greedy action sampling\n",
        "EPS_END = 0.0001                # Final epsilon value\n",
        "EPS_DECAY = 4 * MEM_SIZE        # Amount of samples we decay epsilon over\n",
        "MEM_RETAIN = 0.15                # Percentage of initial samples in replay memory to keep - for catastrophic forgetting\n",
        "NETWORK_UPDATE_ITERS = 5000     # Number of samples 'C' for slowly updating the target network \\hat{Q}'s weights with the policy network Q's weights\n",
        "\n",
        "FC1_DIMS = 128                   # Number of neurons in our MLP's first hidden layer\n",
        "FC2_DIMS = 128                   # Number of neurons in our MLP's second hidden layer\n",
        "\n",
        "# metrics for displaying training status\n",
        "best_reward = 0\n",
        "average_reward = 0\n",
        "episode_history = []\n",
        "episode_reward_history = []\n",
        "np.bool = np.bool_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# for creating the policy and target networks - same architecture\n",
        "class Network(torch.nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.input_shape = env.observation_space.shape\n",
        "        self.action_space = env.action_space.n\n",
        "\n",
        "        # build an MLP with 2 hidden layers\n",
        "        self.layers = torch.nn.Sequential(\n",
        "            torch.nn.Linear(*self.input_shape, FC1_DIMS),   # input layer\n",
        "            torch.nn.ReLU(),     # this is called an activation function\n",
        "            torch.nn.Linear(FC1_DIMS, FC2_DIMS),    # hidden layer\n",
        "            torch.nn.ReLU(),     # this is called an activation function\n",
        "            torch.nn.Linear(FC2_DIMS, self.action_space)    # output layer\n",
        "            )\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE)\n",
        "        self.loss = nn.MSELoss()  # loss function\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# handles the storing and retrival of sampled experiences\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, env):\n",
        "        self.mem_count = 0\n",
        "        self.states = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
        "        self.actions = np.zeros(MEM_SIZE, dtype=np.int64)\n",
        "        self.rewards = np.zeros(MEM_SIZE, dtype=np.float32)\n",
        "        self.states_ = np.zeros((MEM_SIZE, *env.observation_space.shape),dtype=np.float32)\n",
        "        self.dones = np.zeros(MEM_SIZE, dtype=np.bool)\n",
        "\n",
        "    def add(self, state, action, reward, state_, done):\n",
        "        # if memory count is higher than the max memory size then overwrite previous values\n",
        "        if self.mem_count < MEM_SIZE:\n",
        "            mem_index = self.mem_count\n",
        "        else:\n",
        "            mem_index = int(self.mem_count % ((1-MEM_RETAIN) * MEM_SIZE) + (MEM_RETAIN * MEM_SIZE))  # avoid catastrophic forgetting, retain first 10% of replay buffer\n",
        "\n",
        "        self.states[mem_index]  = state\n",
        "        self.actions[mem_index] = action\n",
        "        self.rewards[mem_index] = reward\n",
        "        self.states_[mem_index] = state_\n",
        "        self.dones[mem_index] =  1 - done\n",
        "\n",
        "        self.mem_count += 1\n",
        "\n",
        "    # returns random samples from the replay buffer, number is equal to BATCH_SIZE\n",
        "    def sample(self):\n",
        "        MEM_MAX = min(self.mem_count, MEM_SIZE)\n",
        "        batch_indices = np.random.choice(MEM_MAX, BATCH_SIZE, replace=True)\n",
        "\n",
        "        states  = self.states[batch_indices]\n",
        "        actions = self.actions[batch_indices]\n",
        "        rewards = self.rewards[batch_indices]\n",
        "        states_ = self.states_[batch_indices]\n",
        "        dones   = self.dones[batch_indices]\n",
        "\n",
        "        return states, actions, rewards, states_, dones\n",
        "\n",
        "class DQN_Solver:\n",
        "    def __init__(self, env):\n",
        "        self.memory = ReplayBuffer(env)\n",
        "        self.policy_network = Network(env)  # Q\n",
        "        self.target_network = Network(env)  # \\hat{Q}\n",
        "        self.target_network.load_state_dict(self.policy_network.state_dict())  # initially set weights of Q to \\hat{Q}\n",
        "        self.learn_count = 0    # keep track of the number of iterations we have learnt for\n",
        "\n",
        "    # epsilon greedy\n",
        "    def choose_action(self, observation):\n",
        "        # only start decaying epsilon once we actually start learning, i.e. once the replay memory has REPLAY_START_SIZE\n",
        "        if self.memory.mem_count > REPLAY_START_SIZE:\n",
        "            eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
        "                math.exp(-1. * self.learn_count / EPS_DECAY)\n",
        "        else:\n",
        "            eps_threshold = 1.0\n",
        "        # if we rolled a value lower than epsilon sample a random action\n",
        "        if random.random() < eps_threshold:\n",
        "            # return np.random.choice(np.array(range(9)))   \n",
        "            return np.random.choice(np.array(range(9)), p=[0.15,0.1,0.15,0.1,0.1,0.1,0.1,0.1,0.1])\n",
        "\n",
        "        # otherwise policy network, Q, chooses action with highest estimated Q-value so far\n",
        "        state = torch.tensor(observation).float().detach()\n",
        "        state = state.unsqueeze(0)\n",
        "        self.policy_network.eval()  # only need forward pass\n",
        "        with torch.no_grad():       # so we don't compute gradients - save memory and computation\n",
        "            q_values = self.policy_network(state)\n",
        "        return torch.argmax(q_values).item()\n",
        "\n",
        "    # main training loop\n",
        "    def learn(self):\n",
        "        states, actions, rewards, states_, dones = self.memory.sample()  # retrieve random batch of samples from replay memory\n",
        "        states = torch.tensor(states , dtype=torch.float32)\n",
        "        actions = torch.tensor(actions, dtype=torch.long)\n",
        "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
        "        states_ = torch.tensor(states_, dtype=torch.float32)\n",
        "        dones = torch.tensor(dones, dtype=torch.bool)\n",
        "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
        "\n",
        "        self.policy_network.train(True)\n",
        "        q_values = self.policy_network(states)                # get current q-value estimates (all actions) from policy network, Q\n",
        "        q_values = q_values[batch_indices, actions]           # q values for sampled actions only\n",
        "\n",
        "        self.target_network.eval()                            # only need forward pass\n",
        "        with torch.no_grad():                                 # so we don't compute gradients - save memory and computation\n",
        "            q_values_next = self.target_network(states_)      # target q-values for states_ for all actions (target network, \\hat{Q})\n",
        "\n",
        "        q_values_next_max = torch.max(q_values_next, dim=1)[0]  # max q values for next state\n",
        "\n",
        "        q_target = rewards + GAMMA * q_values_next_max * dones  # our target q-value\n",
        "\n",
        "        loss = self.policy_network.loss(q_values, q_target)     # compute loss between estimated q-values (policy network, Q) and target (target network, \\hat{Q})\n",
        "        #compute gradients and update policy network Q weights\n",
        "        self.policy_network.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.policy_network.optimizer.step()\n",
        "        self.learn_count += 1\n",
        "\n",
        "        # set target network \\hat{Q}'s weights to policy network Q's weights every C steps\n",
        "        if  self.learn_count % NETWORK_UPDATE_ITERS == NETWORK_UPDATE_ITERS - 1:\n",
        "            print(\"updating target network\")\n",
        "            self.update_target_network()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "\n",
        "    def returning_epsilon(self):\n",
        "        return self.exploration_rate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argv[0]=\n",
            "argv[0]=\n",
            "numActiveThreads = 0\n",
            "stopping threads\n",
            "Thread with taskId 0 exiting\n",
            "destroy semaphore\n",
            "semaphore destroyed\n",
            "Thread TERMINATED\n",
            "destroy main semaphore\n",
            "main semaphore destroyed\n",
            "finished\n",
            "numActiveThreads = 0\n",
            "btShutDownExampleBrowser stopping threads\n",
            "Thread with taskId 0 exiting\n",
            "Thread TERMINATED\n",
            "destroy semaphore\n",
            "semaphore destroyed\n",
            "destroy main semaphore\n",
            "main semaphore destroyed\n",
            "waiting for buffer to fill. 41/10000\n",
            "waiting for buffer to fill. 451/10000\n",
            "waiting for buffer to fill. 861/10000\n",
            "waiting for buffer to fill. 1271/10000\n",
            "waiting for buffer to fill. 1681/10000\n",
            "waiting for buffer to fill. 2091/10000\n",
            "waiting for buffer to fill. 2501/10000\n",
            "waiting for buffer to fill. 2911/10000\n",
            "waiting for buffer to fill. 3321/10000\n",
            "waiting for buffer to fill. 3731/10000\n",
            "waiting for buffer to fill. 4141/10000\n",
            "waiting for buffer to fill. 4551/10000\n",
            "waiting for buffer to fill. 4961/10000\n",
            "waiting for buffer to fill. 5371/10000\n",
            "waiting for buffer to fill. 5781/10000\n",
            "waiting for buffer to fill. 6191/10000\n",
            "waiting for buffer to fill. 6601/10000\n",
            "waiting for buffer to fill. 7009/10000\n",
            "waiting for buffer to fill. 7419/10000\n",
            "waiting for buffer to fill. 7829/10000\n",
            "waiting for buffer to fill. 8239/10000\n",
            "waiting for buffer to fill. 8649/10000\n",
            "waiting for buffer to fill. 9059/10000\n",
            "waiting for buffer to fill. 9469/10000\n",
            "waiting for buffer to fill. 9879/10000\n",
            "average total reward per episode batch since episode  300 :  -222.54061903115314\n",
            "updating target network\n",
            "average total reward per episode batch since episode  400 :  -391.9077407000872\n",
            "updating target network\n",
            "average total reward per episode batch since episode  500 :  -388.32808956112314\n",
            "average total reward per episode batch since episode  600 :  -383.14784889794754\n",
            "updating target network\n",
            "average total reward per episode batch since episode  700 :  -381.76662567335103\n",
            "updating target network\n",
            "average total reward per episode batch since episode  800 :  -362.55214560545477\n",
            "updating target network\n",
            "average total reward per episode batch since episode  900 :  -374.2893362181442\n",
            "updating target network\n",
            "average total reward per episode batch since episode  1000 :  -317.31570762133987\n",
            "average total reward per episode batch since episode  1100 :  -313.36400694679304\n",
            "updating target network\n",
            "average total reward per episode batch since episode  1200 :  -326.8241312080762\n",
            "updating target network\n",
            "average total reward per episode batch since episode  1300 :  -278.9716035904986\n",
            "updating target network\n",
            "average total reward per episode batch since episode  1400 :  -289.28319799120123\n",
            "average total reward per episode batch since episode  1500 :  -273.2910457605609\n",
            "updating target network\n",
            "average total reward per episode batch since episode  1600 :  -267.57240284050226\n",
            "updating target network\n",
            "average total reward per episode batch since episode  1700 :  -288.23573308583724\n",
            "updating target network\n",
            "average total reward per episode batch since episode  1800 :  -264.33753670438364\n",
            "average total reward per episode batch since episode  1900 :  -242.34030555310778\n",
            "updating target network\n",
            "average total reward per episode batch since episode  2000 :  -225.48670935161152\n",
            "updating target network\n",
            "average total reward per episode batch since episode  2100 :  -210.48041974054587\n",
            "average total reward per episode batch since episode  2200 :  -242.27047251979155\n",
            "updating target network\n",
            "average total reward per episode batch since episode  2300 :  -201.95618032601215\n",
            "average total reward per episode batch since episode  2400 :  -185.07432889532055\n",
            "updating target network\n",
            "average total reward per episode batch since episode  2500 :  -216.5312486421841\n",
            "updating target network\n",
            "average total reward per episode batch since episode  2600 :  -181.76633628560865\n",
            "average total reward per episode batch since episode  2700 :  -208.88382592858108\n",
            "updating target network\n",
            "average total reward per episode batch since episode  2800 :  -195.77188755304525\n",
            "updating target network\n",
            "average total reward per episode batch since episode  2900 :  -190.30463784093092\n",
            "average total reward per episode batch since episode  3000 :  -187.2714840230747\n",
            "updating target network\n",
            "average total reward per episode batch since episode  3100 :  -195.51839571247731\n",
            "average total reward per episode batch since episode  3200 :  -176.0042673879756\n",
            "updating target network\n",
            "average total reward per episode batch since episode  3300 :  -183.86310790879318\n",
            "updating target network\n",
            "average total reward per episode batch since episode  3400 :  -176.31161839698726\n",
            "average total reward per episode batch since episode  3500 :  -184.28381775486483\n",
            "updating target network\n",
            "average total reward per episode batch since episode  3600 :  -175.2564931498243\n",
            "average total reward per episode batch since episode  3700 :  -151.5945172676772\n",
            "updating target network\n",
            "average total reward per episode batch since episode  3800 :  -193.36598894176822\n",
            "updating target network\n",
            "average total reward per episode batch since episode  3900 :  -174.68001544337562\n",
            "average total reward per episode batch since episode  4000 :  -174.01182323032108\n",
            "updating target network\n",
            "average total reward per episode batch since episode  4100 :  -167.0842488053746\n",
            "updating target network\n",
            "average total reward per episode batch since episode  4200 :  -177.9475128862179\n",
            "average total reward per episode batch since episode  4300 :  -159.09317052472153\n",
            "updating target network\n",
            "average total reward per episode batch since episode  4400 :  -161.15782023330297\n",
            "average total reward per episode batch since episode  4500 :  -155.52932005124936\n",
            "updating target network\n",
            "average total reward per episode batch since episode  4600 :  -172.64279764381422\n",
            "updating target network\n",
            "average total reward per episode batch since episode  4700 :  -186.11102713561368\n",
            "average total reward per episode batch since episode  4800 :  -161.30419740080436\n",
            "updating target network\n",
            "average total reward per episode batch since episode  4900 :  -158.09455961968195\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "env = SimpleDrivingEnv()\n",
        "# set manual seeds so we get same behaviour everytime - so that when you change your hyper parameters you can attribute the effect to those changes\n",
        "env.action_space.seed(0)\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "episode_batch_score = 0\n",
        "episode_reward = 0\n",
        "agent = DQN_Solver(env)  # create DQN agent\n",
        "plt.clf()\n",
        "\n",
        "for i in range(EPISODES):\n",
        "    state = env.reset()  # this needs to be called once at the start before sending any actions\n",
        "    while True:\n",
        "        # sampling loop - sample random actions and add them to the replay buffer\n",
        "        action = agent.choose_action(state)\n",
        "        state_, reward, done, info = env.step(action)\n",
        "        agent.memory.add(state, action, reward, state_, done)\n",
        "\n",
        "        # only start learning once replay memory reaches REPLAY_START_SIZE\n",
        "        if agent.memory.mem_count > REPLAY_START_SIZE:\n",
        "            agent.learn()\n",
        "\n",
        "        state = state_\n",
        "        episode_batch_score += reward\n",
        "        episode_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    episode_history.append(i)\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    episode_reward = 0.0\n",
        "\n",
        "    # save our model every batches of 100 episodes so we can load later. (note: you can interrupt the training any time and load the latest saved model when testing)\n",
        "    if i % 100 == 0 and agent.memory.mem_count > REPLAY_START_SIZE:\n",
        "        if i % 500 == 0:\n",
        "            torch.save(agent.policy_network.state_dict(), f\"policies.bak/policy_network_e{i}.pkl\")\n",
        "        print(\"average total reward per episode batch since episode \", i, \": \", episode_batch_score/ float(100))\n",
        "        episode_batch_score = 0\n",
        "    elif agent.memory.mem_count < REPLAY_START_SIZE:\n",
        "        if(i % 10 == 0):\n",
        "            print(f\"waiting for buffer to fill. {agent.memory.mem_count}/{REPLAY_START_SIZE}\")\n",
        "        episode_batch_score = 0\n",
        "\n",
        "torch.save(agent.policy_network.state_dict(), f\"policies.bak/policy_network.pkl\")\n",
        "\n",
        "env.close()\n",
        "plt.plot(episode_history, episode_reward_history)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3aifCJ2wKDJn"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DRI3 not available\n",
            "failed to load driver: zink\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "argv[0]=\n",
            "startThreads creating 1 threads.\n",
            "starting thread 0\n",
            "started thread 0 \n",
            "argc=3\n",
            "argv[0] = --unused\n",
            "argv[1] = \n",
            "argv[2] = --start_demo_name=Physics Server\n",
            "ExampleBrowserThreadFunc started\n",
            "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
            "X11 functions dynamically loaded using dlopen/dlsym OK!\n",
            "Creating context\n",
            "Created GL 3.3 context\n",
            "Direct GLX rendering context obtained\n",
            "Making context current\n",
            "GL_VENDOR=Mesa\n",
            "GL_RENDERER=llvmpipe (LLVM 17.0.6, 256 bits)\n",
            "GL_VERSION=4.5 (Core Profile) Mesa 23.3.6\n",
            "GL_SHADING_LANGUAGE_VERSION=4.50\n",
            "pthread_getconcurrency()=0\n",
            "Version = 4.5 (Core Profile) Mesa 23.3.6\n",
            "Vendor = Mesa\n",
            "Renderer = llvmpipe (LLVM 17.0.6, 256 bits)\n",
            "b3Printf: Selected demo: Physics Server\n",
            "startThreads creating 1 threads.\n",
            "starting thread 0\n",
            "started thread 0 \n",
            "MotionThreadFunc thread started\n",
            "ven = Mesa\n",
            "ven = Mesa\n",
            "[]\n",
            "-223.8895404800817\n",
            "numActiveThreads = 0\n",
            "stopping threads\n",
            "Thread with taskId 0 exiting\n",
            "destroy semaphore\n",
            "semaphore destroyed\n",
            "Thread TERMINATED\n",
            "destroy main semaphore\n",
            "main semaphore destroyed\n",
            "finished\n",
            "numActiveThreads = 0\n",
            "btShutDownExampleBrowser stopping threads\n",
            "Thread with taskId 0 exiting\n",
            "Thread TERMINATED\n",
            "destroy semaphore\n",
            "semaphore destroyed\n",
            "destroy main semaphore\n",
            "main semaphore destroyed\n"
          ]
        }
      ],
      "source": [
        "env = SimpleDrivingEnv(renders=True)\n",
        "episode_history = []\n",
        "episode_reward_history = []\n",
        "agent = Network(env)  # create DQN agent\n",
        "agent.load_state_dict(torch.load(\"policies.bak/policy_network.pkl\"))\n",
        "\n",
        "state = env.reset()\n",
        "frames = []\n",
        "frames.append(env.render())\n",
        "episode_reward = 0\n",
        "for i in range(200):\n",
        "    frames.append(env.render())  # if running locally not necessary unless you want to grab onboard camera image\n",
        "    state = torch.tensor(state).float().detach()\n",
        "    state = state.unsqueeze(0)\n",
        "    agent.eval()  # only need forward pass\n",
        "    with torch.no_grad():  # so we don't compute gradients - save memory and computation\n",
        "        q_values = agent(state)\n",
        "    action = torch.argmax(q_values).item()\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    episode_batch_score += reward\n",
        "    episode_reward += reward\n",
        "\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "episode_history.append(i)\n",
        "episode_reward_history.append(episode_reward)\n",
        "print(frames[0])\n",
        "print(episode_reward)\n",
        "\n",
        "# vid = display_video(frames, framerate=5)\n",
        "# print(vid)\n",
        "\n",
        "env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
